{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOxBbPXwc2ZwC4WrmJBy82X",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/buzzaggwd/NLP/blob/main/npl_lab3_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ibV6eV4Ace2",
        "outputId": "8d0d98b3-a531-4a0a-f0ad-7ac6a4788b9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50, Loss: 0.1582\n",
            "Epoch 2/50, Loss: 0.1582\n",
            "Epoch 3/50, Loss: 0.1582\n",
            "Epoch 4/50, Loss: 0.1582\n",
            "Epoch 5/50, Loss: 0.1582\n",
            "Epoch 6/50, Loss: 0.1582\n",
            "Epoch 7/50, Loss: 0.1582\n",
            "Epoch 8/50, Loss: 0.1582\n",
            "Epoch 9/50, Loss: 0.1582\n",
            "Epoch 10/50, Loss: 0.1582\n",
            "Epoch 11/50, Loss: 0.1582\n",
            "Epoch 12/50, Loss: 0.1582\n",
            "Epoch 13/50, Loss: 0.1582\n",
            "Epoch 14/50, Loss: 0.1582\n",
            "Epoch 15/50, Loss: 0.1582\n",
            "Epoch 16/50, Loss: 0.1582\n",
            "Epoch 17/50, Loss: 0.1582\n",
            "Epoch 18/50, Loss: 0.1582\n",
            "Epoch 19/50, Loss: 0.1582\n",
            "Epoch 20/50, Loss: 0.1582\n",
            "Epoch 21/50, Loss: 0.1582\n",
            "Epoch 22/50, Loss: 0.1582\n",
            "Epoch 23/50, Loss: 0.1582\n",
            "Epoch 24/50, Loss: 0.1582\n",
            "Epoch 25/50, Loss: 0.1582\n",
            "Epoch 26/50, Loss: 0.1582\n",
            "Epoch 27/50, Loss: 0.1582\n",
            "Epoch 28/50, Loss: 0.1582\n",
            "Epoch 29/50, Loss: 0.1582\n",
            "Epoch 30/50, Loss: 0.1582\n",
            "Epoch 31/50, Loss: 0.1582\n",
            "Epoch 32/50, Loss: 0.1582\n",
            "Epoch 33/50, Loss: 0.1582\n",
            "Epoch 34/50, Loss: 0.1582\n",
            "Epoch 35/50, Loss: 0.1582\n",
            "Epoch 36/50, Loss: 0.1582\n",
            "Epoch 37/50, Loss: 0.1582\n",
            "Epoch 38/50, Loss: 0.1582\n",
            "Epoch 39/50, Loss: 0.1582\n",
            "Epoch 40/50, Loss: 0.1582\n",
            "Epoch 41/50, Loss: 0.1582\n",
            "Epoch 42/50, Loss: 0.1582\n",
            "Epoch 43/50, Loss: 0.1582\n",
            "Epoch 44/50, Loss: 0.1582\n",
            "Epoch 45/50, Loss: 0.1582\n",
            "Epoch 46/50, Loss: 0.1582\n",
            "Epoch 47/50, Loss: 0.1582\n",
            "Epoch 48/50, Loss: 0.1582\n",
            "Epoch 49/50, Loss: 0.1582\n",
            "Epoch 50/50, Loss: 0.1582\n",
            "\n",
            "Тест:\n",
            "Контекст: ['далеко-далеко', 'за', 'словесными']\n",
            "Предсказание: рукописи,\n",
            "Ожидалось: горами\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# текст и его токенизация\n",
        "text = \"Далеко-далеко за словесными горами в стране гласных и согласных живут рыбные тексты. Вдали от всех живут они в буквенных домах на берегу Семантика большого языкового океана. Маленький ручеек Даль журчит по всей стране и обеспечивает ее всеми необходимыми правилами. Эта парадигматическая страна, в которой жаренные члены предложения залетают прямо в рот. Даже всемогущая пунктуация не имеет власти над рыбными текстами, ведущими безорфографичный образ жизни. Однажды одна маленькая строчка рыбного текста по имени Lorem ipsum решила выйти в большой мир грамматики. Великий Оксмокс предупреждал ее о злых запятых, диких знаках вопроса и коварных точках с запятой, но текст не дал сбить себя с толку. Он собрал семь своих заглавных букв, подпоясал инициал за пояс и пустился в дорогу. Взобравшись на первую вершину курсивных гор, бросил он последний взгляд назад, на силуэт своего родного города Буквоград, на заголовок деревни Алфавит и на подзаголовок своего переулка Строчка. Грустный риторический вопрос скатился по его щеке и он продолжил свой путь. По дороге встретил текст рукопись. Она предупредила его: «В моей стране все переписывается по несколько раз. Единственное, что от меня осталось, это приставка «и». Возвращайся ты лучше в свою безопасную страну». Не послушавшись рукописи, наш текст продолжил свой путь.\"\n",
        "tokens = text.lower().replace('\\n', ' ').replace('.', '').split()\n",
        "vocab = sorted(set(tokens))\n",
        "word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
        "idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n",
        "\n",
        "vocab_size = len(vocab)\n",
        "embedding_dim = 10\n",
        "context_size = 3\n",
        "\n",
        "data = [word_to_idx[word] for word in tokens]\n",
        "X_indices, y_indices = [], []\n",
        "for i in range(len(data) - context_size):\n",
        "    X_indices.append(data[i:i + context_size])\n",
        "    y_indices.append(data[i + context_size])\n",
        "\n",
        "X_indices = np.array(X_indices)\n",
        "y_indices = np.array(y_indices)\n",
        "\n",
        "class SimpleLanguageModel:\n",
        "    def __init__(self, vocab_size, embed_dim, context_size, learning_rate=0.01):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_dim = embed_dim\n",
        "        self.context_size = context_size\n",
        "        self.lr = learning_rate\n",
        "\n",
        "        np.random.seed(42)\n",
        "        self.W1 = np.random.randn(context_size * embed_dim, 128) * 0.01\n",
        "        self.W2 = np.random.randn(128, vocab_size) * 0.01\n",
        "        self.embeddings = np.random.randn(vocab_size, embed_dim) * 0.01\n",
        "\n",
        "    def forward(self, X_idx):\n",
        "        x_emb = self.embeddings[X_idx]\n",
        "        self.x_flat = x_emb.reshape(-1, self.context_size * self.embed_dim)\n",
        "\n",
        "        self.h = np.tanh(self.x_flat @ self.W1)\n",
        "        logits = self.h @ self.W2\n",
        "        return logits\n",
        "\n",
        "    def train_step(self, X_idx, target_idx):\n",
        "\n",
        "        logits = self.forward(X_idx)\n",
        "\n",
        "        exp_logits = np.exp(logits - np.max(logits, axis=1, keepdims=True))\n",
        "        probs = exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n",
        "\n",
        "        batch_size = target_idx.shape[0]\n",
        "        loss = -np.mean(np.log(probs[np.arange(batch_size), target_idx] + 1e-9))\n",
        "\n",
        "        dlogits = probs.copy()\n",
        "        dlogits[np.arange(batch_size), target_idx] -= 1\n",
        "        dlogits /= batch_size\n",
        "\n",
        "        dW2 = self.h.T @ dlogits\n",
        "        dh = dlogits @ self.W2.T\n",
        "        dW1 = self.x_flat.T @ (dh * (1 - self.h**2))\n",
        "\n",
        "        self.W1 -= self.lr * dW1\n",
        "        self.W2 -= self.lr * dW2\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def train(self, X, y, epochs=100, batch_size=32):\n",
        "        for epoch in range(epochs):\n",
        "            total_loss = 0\n",
        "            indices = np.random.permutation(len(X))\n",
        "\n",
        "            for i in range(0, len(X), batch_size):\n",
        "                batch_X = X[indices[i:i+batch_size]]\n",
        "                batch_y = y[indices[i:i+batch_size]]\n",
        "\n",
        "                loss = self.train_step(batch_X, batch_y)\n",
        "                total_loss += loss\n",
        "\n",
        "            avg_loss = total_loss / len(X)\n",
        "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")\n",
        "\n",
        "model = SimpleLanguageModel(vocab_size, embedding_dim, context_size)\n",
        "model.train(X_indices, y_indices, epochs=50)\n",
        "\n",
        "test_idx = X_indices[0]\n",
        "logits = model.forward(test_idx[np.newaxis, :])\n",
        "predicted_idx = np.argmax(logits)\n",
        "print(\"\\nТест:\")\n",
        "print(\"Контекст:\", [idx_to_word[i] for i in test_idx])\n",
        "print(\"Предсказание:\", idx_to_word[predicted_idx])\n",
        "print(\"Ожидалось:\", idx_to_word[y_indices[0]])"
      ]
    }
  ]
}
