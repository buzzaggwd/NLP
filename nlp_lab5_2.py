# -*- coding: utf-8 -*-
"""nlp_lab5_2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/buzzaggwd/NLP/blob/main/nlp_lab5_2.ipynb
"""

import pandas as pd
import re
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
import math

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
from sklearn.metrics import adjusted_rand_score, v_measure_score
from sklearn.model_selection import train_test_split

import matplotlib.pyplot as plt

nltk.download('stopwords')
nltk.download('wordnet')

PATH_TO_FILE: str = 'fake_news_dataset2.csv'

df = pd.read_csv(PATH_TO_FILE, encoding='utf-8')

print(df)

# p.s. –¥–∞—Ç–∞—Å–µ—Ç –±—ã–ª –Ω—É –æ—á–µ–Ω—å –æ–≥—Ä–æ–º–Ω—ã–π, –ø—Ä–∏—à–ª–æ—Å—å –æ–±—Ä–µ–∑–∞—Ç—å –¥–æ 500 —Å—Ç—Ä–æ–∫, –∑–∞—Ç–æ —è –º–æ–≥—É –µ–≥–æ –æ–±—Ä–∞–±–æ—Ç–∞—Ç—åü•≤

lemmatizer = WordNetLemmatizer()
english_stopwords = stopwords.words('english')

# —É–¥–∞–ª—è–µ–º —Å—Ç–æ–ø-—Å–ª–æ–≤–∞, –¥–µ–ª–∞–µ–º –Ω–∏–∂–Ω–∏–π —Ä–µ–≥–∏—Å—Ç—Ä, —Ä–∞–∑–¥–µ–ª—è–µ–º —Å–ª–æ–≤–∞
def preprocessing(texts: list[str]) -> list[list[str]]:
    processed = []
    for text in texts:
        clean_text = re.sub(r'[^a-z\s]', '', text.lower())
        words = clean_text.split()
        processed.append(words)
    return processed

processed_texts = preprocessing(df['text'].tolist())
print(processed_texts[10])

# –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è
def lemmatization(texts: list[list[str]]) -> list[list[str]]:
    lemmatized = []
    for sublist in texts:
        current_lemmas = []
        for word in sublist:
            if word not in english_stopwords:
                lemma = lemmatizer.lemmatize(word)
                current_lemmas.append(lemma)
        lemmatized.append(current_lemmas)
    return lemmatized

final_texts = lemmatization(processed_texts)
print(final_texts[10])

# –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è
texts_for_vectorization = [' '.join(tokens) for tokens in final_texts]

# TF-IDF
vectorizer = TfidfVectorizer(max_features=1000)
X = vectorizer.fit_transform(texts_for_vectorization)
print("–ú–∞—Ç—Ä–∏—Ü–∞ TF-IDF (—Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å):", X.shape)

# –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è
n_clusters = df['category'].nunique()
kmeans = KMeans(n_clusters=n_clusters, random_state=42)
clusters = kmeans.fit_predict(X)

# —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å —Ä–µ–∞–ª—å–Ω–æ–π —Ä–∞–∑–º–µ—Ç–∫–æ–π
true_labels = df['category'].values
ari = adjusted_rand_score(true_labels, clusters)
v_measure = v_measure_score(true_labels, clusters)
print(f"–ö–∞—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏:\nARI: {ari:.2f}\nV-measure: {v_measure:.2f}")

# –≤—ã–±–æ—Ä–∫–∏
X_train, X_test, y_train, y_test = train_test_split(X, true_labels, test_size=0.2, random_state=42, stratify=true_labels)
X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42, stratify=y_train)

print("–†–∞–∑–º–µ—Ä—ã –≤—ã–±–æ—Ä–æ–∫:")
print(f"Train: {X_train.shape[0]}, Val: {X_val.shape[0]}, Test: {X_test.shape[0]}")

plt.figure(figsize=(10, 6))
df['category'].value_counts().plot(kind='bar')
plt.title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–π –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ')
plt.xlabel('–ö–∞—Ç–µ–≥–æ—Ä–∏—è –Ω–æ–≤–æ—Å—Ç–µ–π')
plt.ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–æ–≤–æ—Å—Ç–µ–π')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

"""---"""

import tensorflow as tf
print("–î–æ—Å—Ç—É–ø–µ–Ω GPU:", tf.config.list_physical_devices('GPU'))

os.environ["WANDB_DISABLED"] = "true"

!pip install evaluate

!pip install --upgrade transformers datasets

import numpy as np
import torch
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from transformers import (
    DistilBertForSequenceClassification,
    DistilBertTokenizer,
    Trainer,
    TrainingArguments
)
from datasets import Dataset
import os
import evaluate

accuracy_metric = evaluate.load("accuracy")
f1_metric = evaluate.load("f1")

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    return {
        "accuracy": accuracy_metric.compute(predictions=predictions, references=labels)["accuracy"],
        "f1": f1_metric.compute(predictions=predictions, references=labels, average="weighted")["f1"]
    }

le = LabelEncoder()
df['label'] = le.fit_transform(df['category'])

train_df, temp_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['label'])
val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42, stratify=temp_df['label'])

train_dataset = Dataset.from_pandas(train_df[['text', 'label']])
val_dataset = Dataset.from_pandas(val_df[['text', 'label']])
test_dataset = Dataset.from_pandas(test_df[['text', 'label']])

tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')

def tokenize_function(examples):
    return tokenizer(
        examples['text'],
        padding='max_length',
        truncation=True,
        max_length=256
    )

tokenized_train = train_dataset.map(tokenize_function, batched=True)
tokenized_val = val_dataset.map(tokenize_function, batched=True)
tokenized_test = test_dataset.map(tokenize_function, batched=True)

model = DistilBertForSequenceClassification.from_pretrained(
    'distilbert-base-uncased',
    num_labels=len(le.classes_)
)

training_args = TrainingArguments(
    output_dir='./results',
    num_train_epochs=3,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    logging_steps=50,
    save_steps=500,
    learning_rate=2e-5,
    weight_decay=0.01
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_train,
    eval_dataset=tokenized_val,
    compute_metrics=compute_metrics
)

print("–ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è...")
trainer.train()

print("–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏...")
test_results = trainer.evaluate(tokenized_test)

print("–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ:")
print(f"Loss: {test_results['eval_loss']:.3f}")
print(f"Accuracy: {test_results['eval_accuracy']:.3f}")
print(f"F1-Score: {test_results['eval_f1']:.3f}")

device = "cuda" if torch.cuda.is_available() else "cpu"
model.to(device)

sample_text = test_df.iloc[0]['text']
inputs = tokenizer(
    sample_text,
    return_tensors="pt",
    truncation=True,
    max_length=256
).to(device)

outputs = model(**inputs)
predicted_label = le.inverse_transform(
    [np.argmax(outputs.logits.detach().cpu().numpy())]
)
print(f"–ü—Ä–∏–º–µ—Ä –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è: {predicted_label}")
print(f"–ò—Å—Ç–∏–Ω–Ω–∞—è –º–µ—Ç–∫–∞: {test_df.iloc[0]['category']}")

"""–¢–∞–∫, –Ω—É —Ç–µ–ø–µ—Ä—å –≤—ã–≤–æ–¥—ã. –ß—Ç–æ-—Ç–æ –º–Ω–µ –ø–æ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –º—ã –∏–º–µ–µ–º –Ω–∏–∑–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ –º–æ–¥–µ–ª–∏. Accuracy (18%) –∏ F1-Score (5.5%) —Å–æ–≤—Å–µ–º –Ω–µ–±–æ–ª—å—à–∏–µ, —á—Ç–æ –≥–æ–≤–æ—Ä–∏—Ç —á—Ç–æ –º–æ–¥–µ–ª—å –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏ –Ω–µ —Å–ø–æ—Å–æ–±–Ω–∞ –ø—Ä–∞–≤–∏–ª—å–Ω–æ –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å –Ω–æ–≤–æ—Å—Ç–∏.

–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ "Business" –≤–º–µ—Å—Ç–æ "Sports" –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –º–æ–¥–µ–ª—å –Ω–µ —É–ª–∞–≤–ª–∏–≤–∞–µ—Ç —Å–º—ã—Å–ª–æ–≤—ã–µ —Ä–∞–∑–ª–∏—á–∏—è –º–µ–∂–¥—É –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º–∏.

–í–æ–∑–º–æ–∂–Ω–æ (—Ç–æ—á–Ω–µ–µ —Å–∫–æ—Ä–µ–µ –≤—Å–µ–≥–æ) —É –Ω–∞—Å —Ç–∞–∫–∞—è –Ω–µ—É–º–Ω–∞—è –º–æ–¥–µ–ª—å –∏–∑-–∑–∞ —É—Ä–µ–∑–∞–Ω–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞, –Ω–æ —è –Ω–µ –º–æ–≥–ª–∞ –≤–∑—è—Ç—å –±–æ–ª—å—à–µ, —É –º–µ–Ω—è —Ç—É–ø–æ —É–º–∏—Ä–∞–ª –∫–æ–ª–ª–∞–±.. –ü–æ–ª—É—á–∏–ª–æ—Å—å –≤—Å–µ–≥–æ 500 –ø—Ä–∏–º–µ—Ä–æ–≤ –Ω–∞ 7 –∫–ª–∞—Å—Å–æ–≤ (~70 –Ω–∞ –∫–ª–∞—Å—Å) –º–æ–¥–µ–ª–∏ —Å–ª–æ–∂–Ω–æ –≤—ã—è–≤–∏—Ç—å –ø–∞—Ç—Ç–µ—Ä–Ω—ã.
"""